{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "# =================\n",
    "import os\n",
    "import pandas as pd\n",
    "from src.parameters import Parameters\n",
    "from src.external_data import URLConsumer, APIConsumer, ScrappingConsumer\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import warnings;\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_vble      = 'trm'\n",
    "df_trm          = pd.read_csv(os.path.join(Parameters.raw_path, f'df_{macro_vble}.csv'), sep='|')\n",
    "initial_date    =  (pd.to_datetime(df_trm.año_mes.max()) + relativedelta(months=1)).strftime(\"%Y-%m-01\")\n",
    "final_date      = (datetime.now().replace(day=1) - timedelta(days=1)).strftime(\"%Y-%m-01\")\n",
    "final_date      = (datetime.now().replace(day=1) + timedelta(days=1)).strftime(\"%Y-%m-01\")\n",
    "date_run        = datetime.now().strftime(\"%Y-%m\")\n",
    "URLConsumer     = URLConsumer(initial_date, final_date)\n",
    "df_trm_prom_m   = URLConsumer.trm_prom_month()\n",
    "df_trm = pd.concat([df_trm, df_trm_prom_m.reset_index()], axis = 0 )\n",
    "df_trm.to_csv(os.path.join(Parameters.raw_path, f'df_trm.csv'), sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= trm_og =======\n",
      "Number of models compared: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:00<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'lasso__alpha': 599.4842503189421}\n",
      "  Backtesting metric: 101554.1006386136\n",
      "\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(1,0,1)[12] intercept   : AIC=inf, Time=0.59 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=1645.238, Time=0.01 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=1645.844, Time=0.39 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=1645.428, Time=0.31 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=1646.761, Time=0.00 sec\n",
      " ARIMA(0,1,0)(1,0,0)[12] intercept   : AIC=1646.346, Time=0.23 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12] intercept   : AIC=1646.096, Time=0.14 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=1645.288, Time=0.02 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12] intercept   : AIC=1645.209, Time=0.11 sec\n",
      " ARIMA(0,1,1)(1,0,0)[12] intercept   : AIC=1645.734, Time=0.49 sec\n",
      " ARIMA(0,1,1)(1,0,1)[12] intercept   : AIC=inf, Time=0.46 sec\n",
      " ARIMA(1,1,1)(0,0,0)[12] intercept   : AIC=1646.523, Time=0.21 sec\n",
      " ARIMA(0,1,2)(0,0,0)[12] intercept   : AIC=1647.186, Time=0.12 sec\n",
      " ARIMA(1,1,2)(0,0,0)[12] intercept   : AIC=1649.118, Time=0.16 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12]             : AIC=1646.051, Time=0.02 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,0,0)[12] intercept\n",
      "Total fit time: 3.291 seconds\n",
      "Number of models compared: 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:09<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3 4 5 6] \n",
      "  Parameters: {'max_depth': 10, 'n_estimators': 500}\n",
      "  Backtesting metric: 523130.03377712134\n",
      "\n",
      "Number of models compared: 54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:11<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "  Backtesting metric: 715326.4061108576\n",
      "\n",
      "Epoch 1/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 8487532.0000 - val_loss: 13504824.0000\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5697666.5000 - val_loss: 9674207.0000\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 4091417.2500 - val_loss: 6865856.5000\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2918401.2500 - val_loss: 5024956.0000\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2119729.0000 - val_loss: 3528810.2500\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1473377.7500 - val_loss: 2384772.2500\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 981274.0000 - val_loss: 1512883.3750\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 603032.3125 - val_loss: 859682.5000\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 329354.2500 - val_loss: 408520.3750\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 150700.1094 - val_loss: 163828.4375\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 57571.0234 - val_loss: 55907.7812\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 18220.4062 - val_loss: 13024.5352\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3894.8059 - val_loss: 2013.0972\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 543.7841 - val_loss: 181.9315\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 43.3691 - val_loss: 7.5531\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.4257 - val_loss: 0.2333\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0251 - val_loss: 0.0299\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0401\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0460\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0250\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0593\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0496\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0404\n",
      "======= trm_scal01 =======\n",
      "Number of models compared: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'lasso__alpha': 0.0001291549665014884}\n",
      "  Backtesting metric: 0.011395184947217913\n",
      "\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(1,0,1)[12] intercept   : AIC=inf, Time=0.37 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=-503.439, Time=0.01 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=-502.851, Time=0.12 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=-503.267, Time=0.15 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=-501.916, Time=0.01 sec\n",
      " ARIMA(0,1,0)(1,0,0)[12] intercept   : AIC=-502.339, Time=0.15 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12] intercept   : AIC=-502.589, Time=0.07 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=-503.399, Time=0.02 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12] intercept   : AIC=-503.477, Time=0.07 sec\n",
      " ARIMA(0,1,1)(1,0,0)[12] intercept   : AIC=-502.960, Time=0.19 sec\n",
      " ARIMA(0,1,1)(1,0,1)[12] intercept   : AIC=inf, Time=0.70 sec\n",
      " ARIMA(1,1,1)(0,0,0)[12] intercept   : AIC=-501.494, Time=0.03 sec\n",
      " ARIMA(0,1,2)(0,0,0)[12] intercept   : AIC=-501.498, Time=0.12 sec\n",
      " ARIMA(1,1,2)(0,0,0)[12] intercept   : AIC=-499.503, Time=0.11 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12]             : AIC=-502.628, Time=0.03 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,0,0)[12] intercept\n",
      "Total fit time: 2.148 seconds\n",
      "Number of models compared: 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:09<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3] \n",
      "  Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
      "  Backtesting metric: 0.057239593406382405\n",
      "\n",
      "Number of models compared: 54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:07<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "  Backtesting metric: 0.07770119333744246\n",
      "\n",
      "Epoch 1/1000\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1633 - val_loss: 0.6111\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1387 - val_loss: 0.5431\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1170 - val_loss: 0.4682\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0941 - val_loss: 0.3809\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0673 - val_loss: 0.2905\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0469 - val_loss: 0.2229\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0327 - val_loss: 0.1647\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0216 - val_loss: 0.1156\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0889\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0650\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0515\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0396\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0321\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0252\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0159\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 9.1943e-04 - val_loss: 0.0115\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 6.4503e-04 - val_loss: 0.0111\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.5320e-04 - val_loss: 0.0090\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0319e-04 - val_loss: 0.0093\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.8611e-04 - val_loss: 0.0070\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1105e-04 - val_loss: 0.0065\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.8578e-04 - val_loss: 0.0074\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.2578e-04 - val_loss: 0.0062\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.9277e-05 - val_loss: 0.0059\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.2206e-04 - val_loss: 0.0073\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.1113e-04 - val_loss: 0.0061\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1492e-04 - val_loss: 0.0066\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.7479e-04 - val_loss: 0.0070\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.7952e-04 - val_loss: 0.0060\n",
      "======= trm_scal11 =======\n",
      "Number of models compared: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:00<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'lasso__alpha': 0.0016681005372000592}\n",
      "  Backtesting metric: 0.15436195444503498\n",
      "\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(1,0,1)[12] intercept   : AIC=inf, Time=0.58 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=-152.563, Time=0.01 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=-151.974, Time=0.30 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=-152.391, Time=0.14 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=-151.040, Time=0.02 sec\n",
      " ARIMA(0,1,0)(1,0,0)[12] intercept   : AIC=-151.463, Time=0.09 sec\n",
      " ARIMA(0,1,0)(0,0,1)[12] intercept   : AIC=-151.713, Time=0.06 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=-152.523, Time=0.02 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12] intercept   : AIC=-152.601, Time=0.05 sec\n",
      " ARIMA(0,1,1)(1,0,0)[12] intercept   : AIC=-152.084, Time=0.13 sec\n",
      " ARIMA(0,1,1)(1,0,1)[12] intercept   : AIC=inf, Time=0.50 sec\n",
      " ARIMA(1,1,1)(0,0,0)[12] intercept   : AIC=-151.281, Time=0.10 sec\n",
      " ARIMA(0,1,2)(0,0,0)[12] intercept   : AIC=-150.622, Time=0.08 sec\n",
      " ARIMA(1,1,2)(0,0,0)[12] intercept   : AIC=-150.559, Time=0.27 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12]             : AIC=-151.752, Time=0.02 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,0,0)[12] intercept\n",
      "Total fit time: 2.381 seconds\n",
      "Number of models compared: 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:09<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3] \n",
      "  Parameters: {'max_depth': 3, 'n_estimators': 100}\n",
      "  Backtesting metric: 0.8085460040843983\n",
      "\n",
      "Number of models compared: 54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:07<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "  Backtesting metric: 1.0661655241497963\n",
      "\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 33ms/step - loss: 0.6021 - val_loss: 1.7285\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3969 - val_loss: 1.3875\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2807 - val_loss: 1.2223\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1977 - val_loss: 1.0241\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1281 - val_loss: 0.8382\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0797 - val_loss: 0.6853\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0493 - val_loss: 0.5080\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0295 - val_loss: 0.3798\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0165 - val_loss: 0.3015\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.2115\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.1564\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.1237\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.1104\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0879\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0773\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0618\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 0.0436\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0010 - val_loss: 0.0551\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0394\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.8245e-04 - val_loss: 0.0332\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 6.0075e-04 - val_loss: 0.0317\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0261\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0170\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0944e-04 - val_loss: 0.0182\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1494e-04 - val_loss: 0.0160\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 0.0085\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.9046e-04 - val_loss: 0.0153\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.6799e-04 - val_loss: 0.0141\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.0083e-04 - val_loss: 0.0140\n",
      "======= trm_scallg =======\n",
      "Number of models compared: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:00<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'lasso__alpha': 1e-05}\n",
      "  Backtesting metric: 0.002730622540831082\n",
      "\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(1,0,1)[12] intercept   : AIC=inf, Time=0.60 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12] intercept   : AIC=-649.056, Time=0.01 sec\n",
      " ARIMA(1,1,0)(1,0,0)[12] intercept   : AIC=-649.326, Time=0.36 sec\n",
      " ARIMA(0,1,1)(0,0,1)[12] intercept   : AIC=-649.881, Time=0.35 sec\n",
      " ARIMA(0,1,0)(0,0,0)[12]             : AIC=-646.685, Time=0.01 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12] intercept   : AIC=-650.204, Time=0.04 sec\n",
      " ARIMA(0,1,1)(1,0,0)[12] intercept   : AIC=-649.682, Time=0.10 sec\n",
      " ARIMA(0,1,1)(1,0,1)[12] intercept   : AIC=-647.559, Time=0.44 sec\n",
      " ARIMA(1,1,1)(0,0,0)[12] intercept   : AIC=-648.403, Time=0.07 sec\n",
      " ARIMA(0,1,2)(0,0,0)[12] intercept   : AIC=-648.450, Time=0.07 sec\n",
      " ARIMA(1,1,0)(0,0,0)[12] intercept   : AIC=-649.906, Time=0.03 sec\n",
      " ARIMA(1,1,2)(0,0,0)[12] intercept   : AIC=-646.456, Time=0.26 sec\n",
      " ARIMA(0,1,1)(0,0,0)[12]             : AIC=-648.857, Time=0.02 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,0,0)[12] intercept\n",
      "Total fit time: 2.368 seconds\n",
      "Number of models compared: 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:09<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [1 2 3] \n",
      "  Parameters: {'max_depth': 3, 'n_estimators': 100}\n",
      "  Backtesting metric: 0.017771079007042563\n",
      "\n",
      "Number of models compared: 54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lags grid: 100%|██████████| 3/3 [00:07<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12] \n",
      "  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "  Backtesting metric: 0.02537066240957631\n",
      "\n",
      "Epoch 1/1000\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 83.7030 - val_loss: 75.2305\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 67.0259 - val_loss: 64.0615\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 57.7802 - val_loss: 56.4444\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 50.8169 - val_loss: 48.2137\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 43.0054 - val_loss: 40.6679\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 36.1235 - val_loss: 34.0773\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 30.1987 - val_loss: 28.2085\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 25.0700 - val_loss: 23.5373\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 20.7650 - val_loss: 19.1038\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 16.7050 - val_loss: 15.0354\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 12.9930 - val_loss: 11.3503\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 9.6009 - val_loss: 8.0736\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 6.6783 - val_loss: 5.3228\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.2918 - val_loss: 3.2060\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.4980 - val_loss: 1.7055\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.2690 - val_loss: 0.7719\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5392 - val_loss: 0.2824\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1801 - val_loss: 0.0775\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0436 - val_loss: 0.0156\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0024\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 7.0591e-04 - val_loss: 5.0587e-04\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 8.3822e-05 - val_loss: 2.4245e-04\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.0448e-05 - val_loss: 1.7759e-04\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.9973e-05 - val_loss: 2.2690e-04\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.0280e-05 - val_loss: 1.8341e-04\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.0289e-05 - val_loss: 1.1619e-04\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.4101e-05 - val_loss: 1.1863e-04\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 4.9691e-05 - val_loss: 2.4144e-04\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.5801e-05 - val_loss: 3.1164e-04\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 9.8030e-05 - val_loss: 8.2784e-04\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.2624e-04 - val_loss: 0.0087\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0242 - val_loss: 0.0310\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0128 - val_loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow INFO  Assets written to: ram://648506de-43cd-47d2-bff2-c5c8576ce98c/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "matplotlib.category INFO  Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    }
   ],
   "source": [
    "# Modulos\n",
    "# =================\n",
    "from src.pred_preprocessing import clean_atipicis, scalated_dataframe\n",
    "from src.pred_predictions import fn_prediction, plot_pred\n",
    "\n",
    "# Parametros\n",
    "# =================\n",
    "# General\n",
    "steps_test             = 12\n",
    "steps_pred             = 2\n",
    "col_pronos             = 'trm'\n",
    "col_y                  = 'año_mes'\n",
    "scales                 = [\n",
    "                        col_pronos+'_og',\n",
    "                        col_pronos+'_scal01',\n",
    "                        col_pronos+'_scal11',\n",
    "                        col_pronos+'_scallg'\n",
    "                        ]\n",
    "\n",
    "# Process\n",
    "# =================\n",
    "df_macro_vble          = pd.read_csv(os.path.join(Parameters.raw_path, f'df_{col_pronos}.csv'), sep='|')\n",
    "df_clean_atipics = clean_atipicis(df_macro_vble, col_pronos, col_y)\n",
    "df_standarized   = scalated_dataframe(df_clean_atipics, col_pronos)\n",
    "df_pred = fn_prediction(df_standarized, steps_test, scales, col_pronos, col_y, steps_pred)\n",
    "plot_pred(df_macro_vble, df_pred, col_pronos, col_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año_mes</th>\n",
       "      <th>trm</th>\n",
       "      <th>trm_with_atipic</th>\n",
       "      <th>atipic_trm</th>\n",
       "      <th>trm_scal01</th>\n",
       "      <th>trm_scal11</th>\n",
       "      <th>trm_scallg</th>\n",
       "      <th>trm_og</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>1805.523226</td>\n",
       "      <td>1805.523226</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>-1.675274</td>\n",
       "      <td>8.182055</td>\n",
       "      <td>1805.523226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>1802.746333</td>\n",
       "      <td>1802.746333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>-1.678665</td>\n",
       "      <td>8.181278</td>\n",
       "      <td>1802.746333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>1804.400968</td>\n",
       "      <td>1804.400968</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>-1.676645</td>\n",
       "      <td>8.181741</td>\n",
       "      <td>1804.400968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>1821.013667</td>\n",
       "      <td>1821.013667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016925</td>\n",
       "      <td>-1.656363</td>\n",
       "      <td>8.186377</td>\n",
       "      <td>1821.013667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>1792.494839</td>\n",
       "      <td>1792.494839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>-1.691180</td>\n",
       "      <td>8.178405</td>\n",
       "      <td>1792.494839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>4042.797000</td>\n",
       "      <td>4042.797000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.749355</td>\n",
       "      <td>1.056089</td>\n",
       "      <td>8.667933</td>\n",
       "      <td>4042.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>4036.921935</td>\n",
       "      <td>4036.921935</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>1.048916</td>\n",
       "      <td>8.666922</td>\n",
       "      <td>4036.921935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>4065.546452</td>\n",
       "      <td>4065.546452</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756855</td>\n",
       "      <td>1.083862</td>\n",
       "      <td>8.671838</td>\n",
       "      <td>4065.546452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>4160.310000</td>\n",
       "      <td>4160.310000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788095</td>\n",
       "      <td>1.199554</td>\n",
       "      <td>8.687945</td>\n",
       "      <td>4160.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>4178.300000</td>\n",
       "      <td>4178.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794025</td>\n",
       "      <td>1.221517</td>\n",
       "      <td>8.690974</td>\n",
       "      <td>4178.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        año_mes          trm  trm_with_atipic  atipic_trm  trm_scal01  \\\n",
       "0    2012-08-01  1805.523226      1805.523226           0    0.011818   \n",
       "1    2012-09-01  1802.746333      1802.746333           0    0.010903   \n",
       "2    2012-10-01  1804.400968      1804.400968           0    0.011449   \n",
       "3    2012-11-01  1821.013667      1821.013667           0    0.016925   \n",
       "4    2012-12-01  1792.494839      1792.494839           0    0.007524   \n",
       "..          ...          ...              ...         ...         ...   \n",
       "142  2024-06-01  4042.797000      4042.797000           0    0.749355   \n",
       "143  2024-07-01  4036.921935      4036.921935           0    0.747419   \n",
       "144  2024-08-01  4065.546452      4065.546452           0    0.756855   \n",
       "145  2024-09-01  4160.310000      4160.310000           0    0.788095   \n",
       "146  2024-10-01  4178.300000      4178.300000           0    0.794025   \n",
       "\n",
       "     trm_scal11  trm_scallg       trm_og  \n",
       "0     -1.675274    8.182055  1805.523226  \n",
       "1     -1.678665    8.181278  1802.746333  \n",
       "2     -1.676645    8.181741  1804.400968  \n",
       "3     -1.656363    8.186377  1821.013667  \n",
       "4     -1.691180    8.178405  1792.494839  \n",
       "..          ...         ...          ...  \n",
       "142    1.056089    8.667933  4042.797000  \n",
       "143    1.048916    8.666922  4036.921935  \n",
       "144    1.083862    8.671838  4065.546452  \n",
       "145    1.199554    8.687945  4160.310000  \n",
       "146    1.221517    8.690974  4178.300000  \n",
       "\n",
       "[147 rows x 8 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67527449],\n",
       "       [-1.67866465],\n",
       "       [-1.6766446 ],\n",
       "       [-1.65636307],\n",
       "       [-1.69118013],\n",
       "       [-1.71904255],\n",
       "       [-1.69355709],\n",
       "       [-1.66523246],\n",
       "       [-1.64510974],\n",
       "       [-1.62351305],\n",
       "       [-1.54790855],\n",
       "       [-1.5580498 ],\n",
       "       [-1.55736416],\n",
       "       [-1.53611153],\n",
       "       [-1.57808543],\n",
       "       [-1.53337602],\n",
       "       [-1.51969875],\n",
       "       [-1.48999442],\n",
       "       [-1.39085526],\n",
       "       [-1.41378183],\n",
       "       [-1.51310105],\n",
       "       [-1.5411797 ],\n",
       "       [-1.57575601],\n",
       "       [-1.6116429 ],\n",
       "       [-1.56222194],\n",
       "       [-1.46993647],\n",
       "       [-1.37855535],\n",
       "       [-1.28074588],\n",
       "       [-1.02001028],\n",
       "       [-0.95285832],\n",
       "       [-0.92427294],\n",
       "       [-0.72321237],\n",
       "       [-0.82101941],\n",
       "       [-0.90368625],\n",
       "       [-0.7511509 ],\n",
       "       [-0.54414679],\n",
       "       [-0.20163692],\n",
       "       [-0.13593037],\n",
       "       [-0.3031076 ],\n",
       "       [-0.21532391],\n",
       "       [ 0.08112797],\n",
       "       [ 0.11286205],\n",
       "       [ 0.21634294],\n",
       "       [-0.05977283],\n",
       "       [-0.21977918],\n",
       "       [-0.22492945],\n",
       "       [-0.22572266],\n",
       "       [-0.26461753],\n",
       "       [-0.27466862],\n",
       "       [-0.30946445],\n",
       "       [-0.3032096 ],\n",
       "       [-0.08239552],\n",
       "       [-0.20496195],\n",
       "       [-0.28854647],\n",
       "       [-0.36403442],\n",
       "       [-0.28746386],\n",
       "       [-0.36616494],\n",
       "       [-0.31026096],\n",
       "       [-0.26938046],\n",
       "       [-0.17234099],\n",
       "       [-0.24971692],\n",
       "       [-0.31823051],\n",
       "       [-0.27346117],\n",
       "       [-0.2005635 ],\n",
       "       [-0.2270616 ],\n",
       "       [-0.37745702],\n",
       "       [-0.38761372],\n",
       "       [-0.40265722],\n",
       "       [-0.50233385],\n",
       "       [-0.38569339],\n",
       "       [-0.35167677],\n",
       "       [-0.35700827],\n",
       "       [-0.26746564],\n",
       "       [-0.17577873],\n",
       "       [-0.11343879],\n",
       "       [ 0.01988153],\n",
       "       [ 0.04981253],\n",
       "       [-0.01533528],\n",
       "       [-0.07558333],\n",
       "       [-0.05990082],\n",
       "       [-0.02683937],\n",
       "       [ 0.15473623],\n",
       "       [ 0.10104652],\n",
       "       [ 0.03522265],\n",
       "       [ 0.28443204],\n",
       "       [ 0.27067064],\n",
       "       [ 0.31200186],\n",
       "       [ 0.27313187],\n",
       "       [ 0.24453682],\n",
       "       [ 0.16290958],\n",
       "       [ 0.28482726],\n",
       "       [ 0.85373542],\n",
       "       [ 0.97624099],\n",
       "       [ 0.83071423],\n",
       "       [ 0.63954553],\n",
       "       [ 0.58614877],\n",
       "       [ 0.73894848],\n",
       "       [ 0.698892  ],\n",
       "       [ 0.80092169],\n",
       "       [ 0.62003928],\n",
       "       [ 0.35206446],\n",
       "       [ 0.38282304],\n",
       "       [ 0.45995926],\n",
       "       [ 0.53109066],\n",
       "       [ 0.57749204],\n",
       "       [ 0.68112968],\n",
       "       [ 0.61932631],\n",
       "       [ 0.79558621],\n",
       "       [ 0.85878381],\n",
       "       [ 0.78596547],\n",
       "       [ 0.72717757],\n",
       "       [ 0.88529744],\n",
       "       [ 0.95883201],\n",
       "       [ 1.00337156],\n",
       "       [ 0.9255056 ],\n",
       "       [ 0.76842596],\n",
       "       [ 0.75110344],\n",
       "       [ 1.02713313],\n",
       "       [ 0.91931708],\n",
       "       [ 1.45287387],\n",
       "       [ 1.39752512],\n",
       "       [ 1.52984457],\n",
       "       [ 1.82761829],\n",
       "       [ 1.89756602],\n",
       "       [ 1.96751374],\n",
       "       [ 1.87699286],\n",
       "       [ 1.98431562],\n",
       "       [ 1.9394229 ],\n",
       "       [ 1.66636114],\n",
       "       [ 1.66320488],\n",
       "       [ 1.25932388],\n",
       "       [ 1.0883636 ],\n",
       "       [ 1.09018463],\n",
       "       [ 1.00935924],\n",
       "       [ 1.27539657],\n",
       "       [ 1.05121467],\n",
       "       [ 0.94060874],\n",
       "       [ 0.90031103],\n",
       "       [ 0.92013743],\n",
       "       [ 0.88095159],\n",
       "       [ 0.84265863],\n",
       "       [ 0.84403246],\n",
       "       [ 1.05608869],\n",
       "       [ 1.04891615],\n",
       "       [ 1.08386223],\n",
       "       [ 1.19955378],\n",
       "       [ 1.22151677]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pred_models import Models\n",
    "from src.pred_competition import evaluate_models, model_competition\n",
    "from src.pred_preprocessing import de_escalate\n",
    "df_pred = pd.DataFrame()\n",
    "df_filtered = df_standarized\n",
    "test_index = df_filtered[-steps_test:][col_y]\n",
    "pred_index = (pd.to_datetime(df_filtered[-steps_pred:][col_y]) + pd.DateOffset(months=steps_pred)).dt.strftime('%Y-%m-01').reset_index(drop=True)\n",
    "\n",
    "data_train = df_filtered[:-steps_test]\n",
    "data_test = df_filtered[-steps_test:]\n",
    "\n",
    "dict_metrics = pd.DataFrame()\n",
    "dict_models = {}\n",
    "dict_pred = {}\n",
    "\n",
    "for scale in scales:\n",
    "    df_train = data_train[scale]\n",
    "    df_test = data_test[scale]\n",
    "\n",
    "    df_filtered_de_escalate = df_filtered[col_pronos].values.astype(float).reshape(-1, 1)\n",
    "\n",
    "    dict_metrics_by_scale, \\\n",
    "    dict_models_by_scale, \\\n",
    "    dict_pred_by_scale = evaluate_models(\n",
    "                            steps_test, \n",
    "                            df_train, \n",
    "                            df_test,\n",
    "                            df_filtered_de_escalate, \n",
    "                            col_pronos, \n",
    "                            scale\n",
    "                            )\n",
    "    \n",
    "    dict_metrics = pd.concat([dict_metrics,dict_metrics_by_scale], axis = 0)\n",
    "    dict_models.update(dict_models_by_scale)\n",
    "    dict_pred.update(dict_pred_by_scale)\n",
    "# Selección del modelo\n",
    "dict_metrics = dict_metrics.reset_index(drop=True)\n",
    "_winner_metrics, model = model_competition(\n",
    "                                    dict_metrics, \n",
    "                                    dict_models,dict_pred, \n",
    "                                    test_index)\n",
    "\n",
    "# bm = _winner_metrics.pop('model_name')\n",
    "# if 'neuronalnetwork' in bm:\n",
    "#     pred = Models.pred_neuronalnetwork(df_train, steps_pred, model)\n",
    "# else:\n",
    "#     pred = model.predict(steps_pred)\n",
    "# winner_scale = bm.split(\"_\")[-2]+\"_\"+ bm.split(\"_\")[-1]\n",
    "\n",
    "# pred = de_escalate(pred, col_pronos, winner_scale, df_filtered_de_escalate)\n",
    "\n",
    "# df_col_pronos = pd.DataFrame({col_pronos:col_pronos,col_y:pred_index.tolist(), 'pred':pred})\n",
    "# # pred = pred.reset_index(drop=True)\n",
    "    \n",
    "# df_pred = pd.concat([df_pred, df_col_pronos], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow INFO  Assets written to: ram://161b61d0-4894-4ad0-9cd2-b08a59dd0fbb/assets\n"
     ]
    }
   ],
   "source": [
    "#  Selección del modelo\n",
    "dict_metrics = dict_metrics.reset_index(drop=True)\n",
    "_winner_metrics, model = model_competition(\n",
    "                                    dict_metrics, \n",
    "                                    dict_models,dict_pred, \n",
    "                                    test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "bm = _winner_metrics.pop('model_name')\n",
    "if 'neuronalnetwork' in bm:\n",
    "    pred = Models.pred_neuronalnetwork(df_train, steps_pred, model)\n",
    "else:\n",
    "    pred = model.predict(steps_pred)\n",
    "winner_scale = bm.split(\"_\")[-2]+\"_\"+ bm.split(\"_\")[-1]\n",
    "\n",
    "# pred = de_escalate(pred, col_pronos, winner_scale, df_filtered_de_escalate)\n",
    "\n",
    "# df_col_pronos = pd.DataFrame({col_pronos:col_pronos,col_y:pred_index.tolist(), 'pred':pred})\n",
    "# # pred = pred.reset_index(drop=True)\n",
    "    \n",
    "# df_pred = pd.concat([df_pred, df_col_pronos], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uno\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "scale = winner_scale\n",
    "# df_acumulate = df_filtered[winner_scale].values.astype(float).reshape(-1, 1)\n",
    "df_acumulate = df_filtered_de_escalate\n",
    "if scale == col_pronos+'_scal01' or scale == col_pronos+'_scal11':\n",
    "    if scale == col_pronos+'_scal01':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scale == col_pronos+'_scal11':\n",
    "        scaler = StandardScaler()\n",
    "    scaler.fit(df_acumulate)\n",
    "    try:\n",
    "        pred = scaler.inverse_transform(pred.astype(float).reshape(-1, 1)).reshape(-1)\n",
    "        print(\"uno\")\n",
    "    except:\n",
    "        try:\n",
    "            pred = scaler.inverse_transform(pred.values.reshape(-1, 1)).reshape(-1)\n",
    "            print(\"dos\")\n",
    "            \n",
    "        except:\n",
    "            pred = scaler.inverse_transform(np.array(pred).reshape(-1, 1)).reshape(-1)\n",
    "            print(\"tres\")\n",
    "            \n",
    "                \n",
    "elif scale == col_pronos:\n",
    "    pred = np.array(list(pred))\n",
    "\n",
    "elif scale == col_pronos+'_scallg':\n",
    "    k = abs(df_acumulate.min()) + 1\n",
    "    pred = np.array(list(pred))\n",
    "    pred = np.exp(pred)\n",
    "    pred = pred - k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7704437.40934764],\n",
       "       [7075267.53856641]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(pred.astype(float).reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_macro_forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
